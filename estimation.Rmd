---
title: "Software Estimation with Bayesian Regression"
author: "Chris Davey"
date: "10/23/2016"
output: pdf_document
---

```{r setup, include=FALSE}
require(corrplot)
require(plyr)
require(ggplot2)
require(reshape2)
require("gpairs")
require("GGally")
source("bayes_linear_regression.R")

knitr::opts_chunk$set(echo = TRUE)


source("multiplot.R")


file <- "data/merged_data.csv"
data <- read.csv(file, header=TRUE) 


```

## Overview

This document summarises the procedure of applying linear regression to data collected from a variety of software projects in order to construct a bayesian model for linear regression so as to predict a likely region of hours for a given project and a second model used to predict the likelihood of defects for a given project. 

This document provides an illustration of the process, and it is possible after having acquired further data, to update the model, or introduce additional predictors and evaluate their contribution to the model as well as perform subsequent evaluation of the model against earlier iterations using measures such as information criteria.

Note however the data set available is limited, so this example is provided only as an example of method, additionally the attributes for the data set is also relatively limited, and additional attributes would be worth seeking out depending on data collection capability, if performing a similar task.

There is some considerations to be given to collecting data from multiple sources for software estimation, and these are summarised in the Kocaguneli et al article "When to use Data from other projects for effort estimation" along with an approach to use a distance based weighting determining which examples should be included [1].

The data set that was used in this example was acquired from several resources, these files are located in the __data__ directory are are:

- cosmic.arff and isbsg10.arff

Acquired from the Tera data pages offering additional links to research on empirical methods applied to software engineering processes. The data sets are available at the [cosmic data set](http://openscience.us/repo/effort/isbsg/cosmic.html) page and the [isbg data set](http://openscience.us/repo/effort/isbsg/isbsg10.html) page.

This data is a teaser data set and many of the attributes of interest were undefined, only a subset of the data was used. 
However the same procedure can be applied to the full data set (available commercially from the ISBSG site).  
The original arff files and the converted csv files for this data is checked in with the data directory.

- project_tom_data.csv


The additional data is collected manually from example projects in my role at work. This includes additional information as to number of logical components in each project, as well as test case count. Only the numeric values useful for the analysis are retained, individual client names or project codes have been removed. The source data is ignored from the check-in, and is mixed into the other two data sets, the manual data cannot be identified from the merged data set. 

Both groups of data have only a few common attributes, they were merged togethor by selecting a set of attributes that reflect the following data points.

- __functionPoints__

The function point estimate, in the case of the cosmic and isbg data sets these are produced using one of the function point estimation methods nominated in the data set. In the case of the data set collected manually, this reflects either the number of requirements or number of key test cases. The manual data reflects those features of the project that are significant in having been identified in the requirement set.
The other data sets may have used a more formal approach. However, the number of function points (and indirectly requirements) has a positive correlation with the number of hours and defects that result in the project as shown in the correlation diagram.

For the Tera data sets this was derived from the FS property, in the manual data set this is the requirementCnt property. 

- __teamSize__

The maximum size of the team working on the project. The assumption that the larger the team size, there is increased effort in communication about the intent, design, organisation of work, and testing, and defect fixing to name a few concerns. This will also result in increased overall effort for the project.

For the Tera data sets this was the MTS property, for the manual data set this was the teamSize property.

- __totalDefects__

Total defects is a candidate target variable, however this data set does not contain other attributes beyond function points, team size and estimatedHrs (and most of the estimated hours are unavailable). It would be worth while gathering more data to build into a model to predict the potential for defects in the project. Note, as a rule of thumb, the test team at work have held that roughly 13% of test cases may potentially fail resulting in one or more defects, and projects that start to exhibit higher defect rates generally indicate special attention is required. This is more of a conventional belief that has arisen out of experience with a large number of projects, it is based on a reasonable practice of collating test execution records, and has some amount of practical truth to it. However, it would be worth while putting this assumption to the test, and gathering more metrics around size of the project, number of unit tests, coverage and other details in order to model the risk of defect rates and determine whether quality assurance can be improved with efforts in areas such as unit testing, code review and integration testing, which may be assisted by a quicker develop, deploy and test cycle where the development team are more empowered by a quicker means of obtaining feed back and collecting details on that feedback.

The Tera data set defined multiple properties for defects, and this was obtained as the combination of "TOT_Defects", "TOT_B_Defects" and "TOT_I_Defects". For the manual data set this was derived from the property for "totalJiraDefects". 

- __estimatedHrs__

Where available the data for estimated hours is included, however where it is not available it has been set to 0.

For the Tera cosmic data set this was a combination of the properties, "E_Plan", "E_Design", "E_Specify", "E_Build", "E_Implement", "E_Test". For the isbsg data set this was the "E_Estimate" value. And for the manual data this was the "projBudgetHrs". All estimates are in hours.

- __recordedHrs__

The actual number of hours recorded for the project.

For the Tera data set this was derived from the "N_effort" attribute, and for the manual data set this was derived from the "totalBudgetHrs" attribute. 

Only complete cases of interest are maintained in the merged data set, which unfortunately reduces the size of the merged data set significantly.

However, it would be possible to merge additional data sets should they also contain suitable attributes that would map into the properties above as a minimal analsys.
Further analysis is also worth pursuing if additional attributes were available, and common accross multiple data sets.

Note that there is some speculation that having data available from multiple organisations in aide of effort estimation may not be as an effective means of estimation as having good records from within the same company performing the estimation (refer to [Deng Kefu](http://aut.researchgateway.ac.nz/handle/10292/473) [4]). Hence good data collection on project size, quality metrics, effort estimates and actual effort would prove valuable for estimation purposes in the long run. As this data would reflect the internal processes and practices held by the company undertaking the collection. 

This article demonstrates some methods for analysing this data, unfortunately the conclusions drawn with this limited data set are merely speculative. However, the techniques can be applied where there is sufficient data available.  

## Summary of Available Data

The resulting data set contains the number of samples below:

```{r names, echo=FALSE}
nrow(data)
```

Each attribute has the following summary

```{r summary, echo=FALSE}
summary(data)
```

The following set of scatter plots give an indication of the relation between the 
target column "recordedHrs" and the other attributes.

```{r plotScatter, echo=FALSE}

## columns 1:4 will be the predictors
xcols <- 1:4
# pmBudgetHrs=18
ycol <- c(5)

plotScatter <- function(data, cols, targetCol, doLog=FALSE) {
  old.par <- par(mfrow=c(2,2))
  names <- colnames(data)
  targetName<- names[targetCol]
  
  for(i in cols) {
    name <- names[i]
    if (doLog == TRUE) {
      plot(log(data[,i]), log(data[,targetCol]), ylab=targetName, xlab=name)
    } else {
      plot(data[,targetCol]~data[,i], ylab=targetName, xlab=name)
    }
  }
  par(old.par)
}
plotScatter(data, xcols, ycol[1])
```

In general there is a positive relationship between the target variable recorded hours and the other attributes.

The scatter plots against totalDefects are also shown below.


```{r plotScatter2, echo=FALSE}

## columns 1:4 will be the predictors
xcols2 <- c(1,2,4,5)
# pmBudgetHrs=18
ycol2 <- c(3)

plotScatter(data, xcols2, ycol2[1])
```

The relation between totalDefects and the other attributes may suggest a postive relationship while at the same time, there does appear to be some unusual extreme values visible, especially in the far right hand corner. 

While both scatter plots demonstrate that there is a lack of data (only a few sample points), it would be useful to acquire or gather additional information (either by purchasing data, or implementing a data collection method for software project metrics where possible within the organisation).

The same attributes have the following correlations plotted below

```{r corrplot, echo=FALSE}

temp <- data[,c(xcols,ycol)]
C <- cor(temp)
corrplot(C, type="lower",diag=FALSE)
```

The correlation for each of the attributes is listed below

```{r cortable, echo=FALSE}
C
```

Perhaps unsurprisingly there is a positive correlation between most attributes, but most significantly between teamSize and estimatedHrs, and teamSize and recordedHrs. Perhaps surprisingly there is a negative correlation between totalDefects and estimatedHrs, this may be due to estimates tending to be overly optimistic. 

Under a t-test for a 95 % confidence interval we seek a p-value less than :

```{r, echo=FALSE}
q <- qt(0.025, df=nrow(data)-2, lower.tail=FALSE)
p <- pt( q, df=nrow(data)-2, lower.tail=FALSE)
p
```

in order to reject the null hypothesis that the attributes are uncorrelated. Note in the case where two attributes are correlated this would suggest that they may contain a linear dependence and the regression may benefit from having one of the attributes removed. However for the sake of this example, we will not remove the highly correlated attributes.

The correlation between teamSize and recordedHrs has the p-value 

```{r, echo=FALSE}
test <- cor.test(data$teamSize, data$recordedHrs)
test$p.value
```

and is flagged as indicating that the correlation with recordedHrs is likely 

```{r,echo=FALSE}
test$p.value < p
```

The same procedure can be repeated for the other attributes against the recordedHrs attribute in order 
to determine whether the correlation is feasible.


```{r, echo=FALSE}
# estimate correlation with recordedHrs
attrib <- colnames(data)
attrib1 <- attrib[5]
i <- 5
results <- sapply(1:4, function(a) 0)
for (j in 1:4) {
  test <- cor.test(data[,i],data[,j])
  results[j] <- test$p.value
}
testData <- data.frame(attributes=attrib[1:4], test=results, flag=results < p)
paste("T-Test for correlation of", attrib1, "to other attributes")
testData
```

Interestingly, the functionPoints, and totalDefects from the recorded data do not test in such a manner as to suggest it correlates with recordedHrs, whereas the correlation by itself suggested this relation was positive. 

The same procedure can be applied to gather some indication of relationship between the attributes and
the second target variable, totalDefects.


```{r, echo=FALSE}
# estimate correlation with totalDefects
attrib <- colnames(data)
attrib1 <- attrib[3]
i <- 3
results <- sapply(c(1,2,4,5), function(a) 0)
n <- 1
for (j in c(1,2,4,5)) {
  test <- cor.test(data[,i], data[,j])
  results[n] <- test$p.value
  n <- n + 1
}
testData <- data.frame(attributes=attrib[c(1,2,4,5)], test=results, flag=results < p)
paste("T-Test for correlation of", attrib1, "to other attributes")
testData
```

Which interestingly suggests the totalDefects is not siginificantly correlated with any other attributes, hence may be independent of the other attributes. This independence would identify it as a potentially useful predictor when it is not used as a target variable.

The same method can be expanded to give a summary of linear dependence between the features using a test for significance between MxM features.
```{r, echo=FALSE}
# estimate correlation with totalDefects
attrib <- colnames(data)
# cols by cols comparison
results <- matrix(0, nrow=ncol(data), ncol=ncol(data))
for (i in 1:length(attrib)) {
  for (j in 1:length(attrib)) {
    test <- cor.test(data[,i], data[,j])
    results[i,j] <- test$p.value
  }  
}

testData <- data.frame(results)
rownames(testData) <- attrib
colnames(testData) <- attrib
paste("p-values for test of correlation of attributes")
testData
paste("T-Test for test of correlation of attributes")
testData < p
```

```{r, echo=FALSE}
cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
    diag(p.mat) <- 0
    diag(lowCI.mat) <- diag(uppCI.mat) <- 1
    for(i in 1:(n-1)){
        for(j in (i+1):n){
            tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
            p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
            lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
            uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
        }
    }
    return(list(p.mat, lowCI.mat, uppCI.mat))
}

res1 <- cor.mtest(data,0.95)
## specialized the insignificant value according to the significant level
M <- cor(data)
corrplot(M, p.mat = res1[[1]], sig.level=p)
```


This process is a kind of feature selection, the aim is to remove redundant predictors from 
the attributes, where those attributes are correlated.

Note that estimatedHrs and recordedHrs are strongly correlated and the test confirms this correlation exists, additionally teamSize is positively correlated with both estimatedHrs and recordedHrs and the test indicates this correlation exists. Hence, there is a linear dependence between these three attributes. When estimatedHrs and teamSize are used as predictors it may be recommended
to remove one of these attributes from the predictor columns.

Note also that the method of testing is a point estimate approach, later on when we examine parameter space confidence intervals will be more useful. However so far this approach gives some indication as to the kind of relationship that exists within the data set, and some brief idea as to the ranges available for the values within the data set.

## Modelling Prior and Posterior Distributions

Drawing on the example data available, we will make use of bayes least squares regression. However before applying this to the model, some description of the process being applied in the model is required.

The approach uses a linear regression, given the data we assume that the distribution of $Y$, the predictors $X$ and the parameters $\beta$ and $\sigma^2$ are distributed as
$$
p(Y|\beta,\sigma^2,X) \sim N(X\beta, \sigma^2I)
$$
we are assuming that $Y$ has a normal distribution about the product of $X$ and $\beta$ with a uniform variance $\sigma^2$ ($I$ is the diagonal identity matrix). The conditional distribution of the parameters is assumed to be uniform 
$$
p(\beta, \sigma^2|X) \propto \sigma^{-2}
$$

The posterior distribution of the parameters are given by the rule
$$
\text{posterior} \propto \text{likelihood} \times \text{prior}
$$

The posterior distribution of the parameters becomes

$$
p(\beta,\sigma^2|Y,X) \propto N(X\beta, \sigma^2I) \times \sigma^{-2}
$$
And factors into the product of the multivariate normal distribution and the inverse chi squared distribution
$$
\propto N\left(\hat{\beta}, \sigma^2(X'X)^{-1}\right) \times \text{Inverse-}\chi^2(\nu,s^2)
$$
where the estimate of $\beta$ is
$$
\hat{\beta} = (X'X)^{-1}X'Y
$$
$\nu$ is the degrees of freedom 
$$
\nu = n - k
$$
Note that $X$ is of dimension $n \times k$ and that $n \ge k$.
The estimate of the variance is derived from:
$$
s^2 = (1/\nu)(Y - \bar{Y})'(Y-\bar{Y})
$$
$$
\bar{Y} = X\hat{\beta}
$$
The posterior predictive distribution of new target variable $\tilde{Y}$ given new predictor examples $\tilde{X}$ and the posterior for parameters results as the product of the likelihood of $\tilde{Y}$ given the new data and the posterior distribution of the parameters from the previous data. 
$$
p(\tilde{Y}|\beta,\sigma^2,\tilde{X},Y,X) = f(\tilde{Y}|\beta,\sigma^2,\tilde{X})\times p(\beta,\sigma^2|Y,X)
$$
After marginalising the predictive density $p(\tilde{Y}|Y,\tilde{X},X)$ becomes a multivariate t-distribution centered at $\tilde{X}\hat{\beta}$, scaled by $[I + \tilde{X}V\tilde{X}']s^2$ with $\nu = n - k$ degress of freedom. Note that $V = (X'X)^{-1}$ and $\hat{\beta} = (X'X)^{-1}X'Y = V X'Y$

## Update Rules

When presented with new observations it is also possible to update the distribution of the 
parameters for $\beta$ and $\sigma$ using the following update rules

$$
s_n^2 = s^2 + \frac{1}{n_{new} - k}(Y_{new} - X_{new}\hat{\beta})'(Y_{new} - X_{new}\hat{\beta})
$$
and the new degrees of freedom
$$
\nu_n = \nu + (n_{new} - k)
$$
which yields the parameters for the $\text{Inverse-}\chi^2(\nu_n,s_n^2)$ distribution.
The $\beta$ parameter is updated as
$$
\mu_0 = \hat{\beta} = V_{\beta}X'Y
$$
$$
\Lambda_0 = V_{\beta}
$$
$$
\Lambda_n = \Lambda_0^{-1} + \Sigma^{-1}
$$
$$
u_n = (\Lambda_0^{-1} + \Sigma^{-1})^{-1}(\Lambda_0^{-1}\mu_0 + \Sigma^{-1}\bar{Y}_{new})
$$
and $\Sigma = X_{new}'X_{new}$. The updated distribution for $\beta$ becomes 
$$
p(\beta|\sigma^2,Y_{new},Y,X_{new},X) \sim N(\mu_n, \sigma^2\Lambda_n)
$$
and the updated joint posterior distribution becomes
$$
p(\beta,\sigma^2|Y_{new},Y,X_{new},X) \propto N(\mu_n, \sigma^2\Lambda_n) \times \text{Inverse-}\chi^2(\nu_n,s_n^2)
$$
The new parameter estimates $\mu_n$ and $\sigma^2\Lambda_n$, $\nu_n$ and $s_n^2$ can be substituted into the regression model in order to update the model estimates, as well as used within the predictive distribution. Ongoing model updates can occur in the same procedure. 

The initial effect of the uninformative prior is equivalent to standard linear regression. However one of the  advantages of generating the bayesian model is the ability to perform an update of the parameters when new data becomes available. Details and further references are discussed here at [Notes on Bayes Linear Regression](http://cxd.github.io/scala-au.id.cxd.math/notes/bayesianlinearregression.html) as well as definitive resources to be found in Box and Tiao [2] and Gelman [3].


## Inspecting Boundaries of Predictors with Credible intervals and the Highest Posterior Density


Once the model is trained we can inspect the upper and lower bounds of the parameter space, mainly the upper and lower bounds of the estimated mean of the predictor variables.

The boundary is determined on the marginal posterior distribution of the $\beta$ parameter, which after marginalisation is a multivariate t-distribution centered at $\hat{\beta}$ and scaled by $X'Xs^2$ with $\nu = n - k$ degrees of freedom.

$$
p(\beta|Y,X) \propto \left[1 + \frac{(\beta - \hat{\beta})'X'X(\beta-\hat{\beta})}{\nu s^2}\right]^{-\frac{1}{2}(\nu + k)}
$$

Using the posterior distributions it is then possible to obtain credible intervals for the parameters at a given $\alpha$ level. For the parameter $\beta$ this can be achieved via the multivariate t-distribution $p(\beta|Y,X)$ when the variance is assumed known we can make use of the $t$ values for the $1-\alpha$ level
$$
\hat{\beta} \pm t_{\nu,\alpha/2}\sqrt{(X'X)s^2}
$$

The first plot examines the first two dimensions of the parameter space of the $\beta$ parameter matrix, the other plots examine the confidence intervals for the target variable recordedHrs based on the model at the 95% confidence interval. Note that under the normal distribution the sign of the extremum can be negative, although in the case of the project durations, this value does not realistically apply.

```{r, echo=FALSE}
## predictor length
len=40

```


```{r,echo=T}

## columns 1:4 will be the predictors
xcols <- 1:4
# target column is 5
ycol <- c(5)
test <- bayes_least_squares_uninform_prior(data, xcols, ycol, zscale=TRUE)

```

```{r,echo=FALSE}

## the model is built now estimate the HPD
rBeta1 <- test$randBetaPosterior(test, test$sSqr, len)
d2 <- test$betaConfData(test, test$sSqr,len, names= c("B1", "B2", "B3", "B4", "prob"))

```

```{r,echo=FALSE}

# 95%
betaConf1 <- test$betaConfInterval(test, test$sSqr,0.05)
x0 <- betaConf1$beta[1,1]
y0 <- betaConf1$beta[2,1]
delta <- betaConf1$upperQuartile - betaConf1$lowerQuartile
theta <- seq(0,2*pi,length=len)
x <- x0 + delta*cos(theta)
y <- y0 + delta*sin(theta)

# 90%
betaConf2 <- test$betaConfInterval(test, test$sSqr,0.1)
delta2 <- betaConf2$upperQuartile - betaConf2$lowerQuartile
x2 <- x0 + delta2*cos(theta)
y2 <- y0 + delta2*sin(theta)

#75%
betaConf3 <- test$betaConfInterval(test, test$sSqr,0.25)
delta3 <- betaConf3$upperQuartile - betaConf3$lowerQuartile
x3 <- x0 + delta3*cos(theta)
y3 <- y0 + delta3*sin(theta)

hpd <- data.frame(x95=x,y95=y,x90=x2,y90=y2,x75=x3,y75=y3)


## approximate hpd
old.par <- par(mfrow=c(2,2))
options(scipen=5)
plot(x3,y3,type="l", main="hpd beta1,beta2 at 75%,90% and 95%", xlab="beta1", ylab="beta2")
n <- ncol(rBeta1)
m <- n-1
points(rBeta1[,2]~rBeta1[,1], col="lightblue")
lines(x3,y3)
lines(x2,y2,col="blue")
lines(x,y,col="red")
## scaling up confidence level to Y
Ylower <- test$X%*%betaConf1$betaL
Ylower <- Ylower*test$yscale + test$ycenter

Yupper <- test$X%*%betaConf1$betaU
Yupper <- Yupper*test$yscale + test$ycenter
Ymean <- test$X%*%betaConf1$beta
Ymean <- Ymean*test$yscale + test$ycenter

plotData1 <- data.frame(lower=Ylower, upper=Yupper)

min1 <- min(Ylower, Yupper)
max1 <- max(Ylower, Yupper)
# , 
hist(Ylower, breaks=len/2, col=rgb(0,0,1,1/4), main="95% CI for E[recordedHrs]", xlab="E[recordedHrs]")
hist(Yupper, breaks=len/2, col=rgb(1,0,0,1/4), add=T)

boxplot(plotData1, main="E[recordedHrs] at 95% CI")

plotData2 <- data.frame(mu=Ymean, upper=Yupper, lower=Ylower)
plot(Ymean , type="l", ylab="target hours", main="95% CI Y|X")
lines(Yupper, col="red")
lines(Ylower, col="blue")

par(old.par)
```

The upper confidence interval has a slighly higher distribution than the lower confidence interval. The summary of each being

```{r,echo=FALSE}
print("Summary of upper 95% CI")
summary(Yupper)
print("Summary of lower 95% CI")
summary(Ylower)
```

Using the HPD of the beta interval is it also possible to gauge some 
idea of the distribution of the mean for the predictors this is derived by sampling from the posterior distribution
for the beta-parameter, by doing so we can obtain expected values for the mean of the predictors
by transforming the beta parameters back into the predictor parameters (this is done via scaling and offsetting, which have been stored during the training procedure). It gives an indication of the distribution
that the model has defined for the expected values of the predictors.

```{r}
len <- 100
d2 <- test$betaConfData(test, test$sSqr,len, names= c("B1", "B2", "B3", "B4", "prob"))

rConf = data.frame(b1=d2[,1]*test$xscale[1] + test$xcenter[1],
                   b2=d2[,2]*test$xscale[2] + test$xcenter[2],
                   b3=d2[,3]*test$xscale[3] + test$xcenter[3],
                   b4=d2[,4]*test$xscale[4] + test$xcenter[4])
colnames(rConf) <- colnames(data)[1:4]

```

```{r,echo=FALSE}

#ggpairs(data=rConf,
#        upper=list(continuous="density"),
#        lower=list(params=list(size=2)),
#        title="Beta Density for E[X] over 100 Simulations on the 95% confidence interval",
#        params=list(labelSize=8),
#        columnLabels=colnames(rConf))

## produce box plots for the items
old.par <- par(mfrow=c(1,2))
options(scipen=5)
boxplot(rConf$functionPoints, data$functionPoints, col=topo.colors(2), xlab="functionPoints")
legend("topleft", legend=c("generated", "original"), fill=topo.colors(2))

boxplot(rConf$teamSize, data$teamSize, col=topo.colors(2), xlab="teamSize")
legend("topleft", legend=c("generated", "original"), fill=topo.colors(2))

par(old.par)

```

```{r, echo=FALSE}

old.par <- par(mfrow=c(1,2))

boxplot(rConf$totalDefects, data$totalDefects, col=topo.colors(2), xlab="totalDefects")
legend("topleft", legend=c("generated", "original"), fill=topo.colors(2))

boxplot(rConf$estimatedHrs, data$estimatedHrs, col=topo.colors(2), xlab="estimatedHrs")
legend("topleft", legend=c("generated", "original"), fill=topo.colors(2))

par(old.par)
```

The summary of the confidence intervals for the mean of the predicted values are below, we have two different distributions for the predictor variables, that of the original data source, and that which is generated based
on the expected values for the parameter of the model. We can additionally form upper and lower confidence thresholds for predictor values in the same manner.

```{r,echo=FALSE}
print("Summary of predictor attributes from source data")
summary(data[,1:4])

print("Summary of expected values for predictor attributes")
summary(rConf)
```

This also provides some insight into how the model may be used in a generative manner, where sampling form the parameter space can be used for example in simulation, or generation of representative data sets for the predictor variables, based on the posterior distribution of the parameters. 

## Application to prediction and simulation

Additionally it is possible to demonstrate the use of prediction of the model using the parameters derived from the non-informative prior. The prediction of the current data set is shown in the first plot, and a simulation of the predictive posterior
generated from the multivariate normal distribution from the posterior parameters is shown below.

```{r}
len <- 1000
X1 <- data[,xcols]
Y1 <- data[,ycol]
## investigate the prediction
Ypred <- test$predictY(test, X1)
Ypred1 <- test$yPredictivePosteriorSimulate(test,X1, len)

```

```{r, echo=FALSE}

Ys <- (Y1 - test$ycenter)/test$yscale

residual <- Ypred - Ys
residual1 <- Ypred1 - Ys 

mse1 <- sapply(1:nrow(Ypred), function(i) { sum(((Ypred[i,] - Ys)^2) / nrow(Ypred)) })
mse2 <- sapply(1:nrow(Ypred1), function(i) { sum(((Ypred1[i,] - Ys)^2) / nrow(Ypred1)) })

mseData1 <- data.frame(MSE=mse1, prediction=1:nrow(Ypred))
mseData2 <- data.frame(MSE=mse2, simulation=1:nrow(Ypred1))

minIdx <- which(mse2 %in% min(mse2))


YpredScaled <- Ypred*test$yscale + test$ycenter
Ypred1Scaled <- (Ypred1[minIdx,]*test$yscale) + test$ycenter
residualScaled <- residual1[minIdx,]*test$yscale + test$ycenter
stdResidualScaled <- residual*test$yscale + test$ycenter

plotData <- data.frame(requirementCnt=data$functionPoints, 
                       stdResidual=residual,
                       stdResidualScaled=stdResidualScaled,
                       residuals=residual1[minIdx,],
                       residualScaled=residualScaled,
                       predictedWaitTime=YpredScaled,
                       simulatedWaitTime=Ypred1Scaled,
                       actualHours=Y1)

```


```{r, echo=FALSE}

## plot the prediction

cols <- c("#000000", "#FF0000")
labels <- c("actual", "predicted")

cols2 <- c("#000000", "#FF00FF")
labels2 <- c("actual", "simulated")


pA1 <- ggplot(plotData, aes(x=requirementCnt)) +
  geom_point(aes(y=actualHours, col="actual")) +
  geom_point(aes(y=predictedWaitTime, col="predicted"), shape=1) +
  scale_colour_manual(values=cols, labels=labels) +
  ggtitle("Original Data predicted hours on actual hours")

pA2 <- ggplot(plotData, aes(x=requirementCnt)) +
  geom_point(aes(y=actualHours, col="actual")) +
  geom_point(aes(y=simulatedWaitTime, col="simulated"), shape=1) +
  scale_colour_manual(values=cols2, labels=labels2) +
  ggtitle("Simulated hours on actual hours")

multiplot(pA1,pA2)

```

```{r,echo=FALSE}
boxplot(plotData$predictedWaitTime, plotData$simulatedWaitTime, plotData$actualHours, col=topo.colors(3),
        main="Comparison of recordedHrs target variable")
legend("topleft", legend=c("predicted", "simulated", "original"), fill=topo.colors(3))
```

The simulation generates 1000 draws of the beta distribution and then selects the prediction based on the simulation with the lowest MSE, whereas the prediction performs a single prediction against the original data.

The residual of each approach is shown below.

```{r, echo=FALSE}
# plotting the MSE


p1 <- ggplot(plotData, aes(x=stdResidualScaled)) +
  geom_histogram(color="black", fill="lightblue") + 
  ggtitle("Histogram of residuals for estimate")

p2 <- ggplot(plotData, aes(x=residualScaled)) +
  geom_histogram(color="black", fill="lightblue") + 
  ggtitle("Histogram of residuals for simulation")


p3 <- ggplot(plotData, aes(x=stdResidual)) +
  geom_histogram(color="black", fill="lightblue") + 
  ggtitle("Histogram of residuals for estimate")

p4 <- ggplot(plotData, aes(x=residuals)) +
  geom_histogram(color="black", fill="lightblue") + 
  ggtitle("Histogram of residuals for simulation")

multiplot(p1, p2)

```


```{r,echo=FALSE}

multiplot(p3, p4)

```

```{r,echo=FALSE}

scaleF <- function(y) {
  y*test$yscale + test$ycenter
}

ms1 <- sum((Ypred - Ys)^2) / nrow(Ypred)
scaledms1 <- sum((scaleF(t(Ypred)) - scaleF(Ys))^2) / nrow(Ypred)
ms2 <- sum((Ypred1[minIdx,] - Ys)^2) / nrow(Ypred1)
scaledms2 <- sum((scaleF(Ypred1[minIdx,]) - scaleF(Ys))^2) / nrow(Ypred1)

print("MSE for prediction and simulation, both unscaled and scaled")
model1Mse <- data.frame(mse1=ms1, scaledmse1=scaledms1, simmse2=ms2, simscaledmse2=scaledms2)
model1Mse
```

Based on the MSE alone, it suggests that it is possible to gain a more accurate estimate through the use of simulation, although the distribution of the simulation may be quite varied and would require a selection criteria for evaluating each simulation. It may be possible using the confidence bounds of the distribution to find a simulation that matches closely against the mean for the expected distribution in calculating an error, instead of the target variable directly.

## Updating the Model.

In the case where we have additional observations it is possible to update the model. Unfortunately in the case
of the software estimation domain we do not have available to us additional data, as further data collection
is required, however in order to demonstrate the procedure, it is possible to use the model in a generative
manner to create both the new dataset and the new target variable. This generated data set is drawn from the
parameter space in the same manner as described above.

```{r}
len <- 100
d2 <- test$betaConfData(test, test$sSqr,len, names= c("B1", "B2", "B3", "B4", "prob"))

newData = data.frame(b1=d2[,1]*test$xscale[1] + test$xcenter[1],
                   b2=d2[,2]*test$xscale[2] + test$xcenter[2],
                   b3=d2[,3]*test$xscale[3] + test$xcenter[3],
                   b4=d2[,4]*test$xscale[4] + test$xcenter[4])
colnames(newData) <- colnames(data)[1:4]
newY <- test$predictY(test, newData)
newY <- newY*test$yscale + test$ycenter
newData <- cbind(newData, data.frame(recordedHrs=newY))
```

Interestingly the generated data for the target variable appears to have a negative relationship with the estimated Hrs attribute, however appears to demonstrate a strong linear relationship with the teamSize, whilst only a slight linear relation with functionPoints, and even less of a relation with totalDefects. 

```{r, echo=FALSE}
plotScatter(newData, 1:4, 5)
summary(newData)
```

In order to update the model the new data is applied as follows.


```{r}
Xnew <- as.matrix(newData[,1:4])
Ynew <- as.matrix(data.frame(Y=newData[,5]))
updateModel <- test$applyUpdateRules(test, Ynew, Xnew)

```


Once the model has been updated we can also examine the confidence interval for the target variable

```{r, echo=FALSE}
# 95%
NbetaConf1 <- updateModel$betaConfInterval(updateModel, updateModel$sSqr,0.05)

Ylower2 <- updateModel$X%*%NbetaConf1$betaL
Ylower2 <- Ylower2*updateModel$yscale + updateModel$ycenter

Yupper2 <- updateModel$X%*%NbetaConf1$betaU
Yupper2 <- Yupper2*updateModel$yscale + updateModel$ycenter
Ymean2 <- updateModel$X%*%NbetaConf1$beta
Ymean2 <- Ymean2*updateModel$yscale + updateModel$ycenter

plotData2 <- data.frame(mu=Ymean, upper=Yupper, lower=Ylower)

plotData3 <- data.frame(lower=Ylower2[,1], upper=Yupper2[,1])

min1 <- min(Ylower, Yupper)
max1 <- max(Ylower, Yupper)
# , 
hist(Ylower2, breaks=len/2, col=rgb(0,0,1,1/4), main="95% CI for E[recordedHrs]", xlab="E[recordedHrs]")
hist(Yupper2, breaks=len/2, col=rgb(1,0,0,1/4), add=T)

```

```{r,echo=FALSE}

boxplot(plotData2$lower, plotData2$upper, plotData3$lower, plotData3$upper, data$recordedHrs, main="E[recordedHrs] at 95% CI",
        col=rainbow(5), names=c("mode1lower", "model1upper", "model2lower", "model2upper", "original"))
legend("topleft", legend=c("mode1lower", "model1upper", "model2lower", "model2upper", "original"), fill=rainbow(5))
```

```{r, echo=FALSE}
print("Summary of original recordedHrs")
summary(data$recordedHrs)

print("Summary of original model CI recordedHrs")
summary(plotData2)


print("Summary of updated model CI recordedHrs")
summary(plotData3)
```

The distribution of the expected target variable has narrowed somewhat, this effect may be expected due to the use of data that was generated from the previous model.

Similarly the effect can be observed on the expected distributions of the predictors.


```{r}
len <- 100
d2 <- updateModel$betaConfData(updateModel, updateModel$sSqr,len, names= c("B1", "B2", "B3", "B4", "prob"))

rConf2 = data.frame(b1=d2[,1]*updateModel$xscale[1] + updateModel$xcenter[1],
                   b2=d2[,2]*updateModel$xscale[2] + updateModel$xcenter[2],
                   b3=d2[,3]*updateModel$xscale[3] + updateModel$xcenter[3],
                   b4=d2[,4]*updateModel$xscale[4] + updateModel$xcenter[4])
colnames(rConf2) <- colnames(data)[1:4]

```

```{r,echo=FALSE}

#ggpairs(data=rConf,
#        upper=list(continuous="density"),
#        lower=list(params=list(size=2)),
#        title="Beta Density for E[X] over 100 Simulations on the 95% confidence interval",
#        params=list(labelSize=8),
#        columnLabels=colnames(rConf))

## produce box plots for the items
old.par <- par(mfrow=c(1,2))
options(scipen=5)
boxplot(rConf$functionPoints, rConf2$functionPoints, data$functionPoints, col=topo.colors(3), xlab="functionPoints")
legend("topleft", legend=c("1stmodel", "2ndmodel", "original"), fill=topo.colors(3))

boxplot(rConf$teamSize, rConf2$teamSize, data$teamSize, col=topo.colors(3), xlab="teamSize")
legend("topleft", legend=c("1stmodel", "2ndmodel", "original"), fill=topo.colors(3))

par(old.par)

```

```{r, echo=FALSE}

old.par <- par(mfrow=c(1,2))

boxplot(rConf$totalDefects, rConf2$totalDefects, data$totalDefects, col=topo.colors(3), xlab="totalDefects")
legend("topleft", legend=c("1stmodel", "2ndmodel", "original"), fill=topo.colors(3))

boxplot(rConf$estimatedHrs, rConf2$estimatedHrs, data$estimatedHrs, col=topo.colors(3), xlab="estimatedHrs")
legend("topleft", legend=c("1stmodel", "2ndmodel", "original"), fill=topo.colors(3))

par(old.par)
```

Interestingly there is some broadening of the predictor distribution visible as a result of the update.

In comparing the two models it is possible to evaluate predictions against the original data.


```{r}
len <- 1000
X1 <- data[,xcols]
Y1 <- data[,ycol]

functionPoints <- data$functionPoints

## investigate the prediction
Ypred2 <- updateModel$predictY(updateModel, X1)
Ypred3 <- updateModel$yPredictivePosteriorSimulate(updateModel,X1, len)

```

```{r, echo=FALSE}

Ys2 <- (Y1 - updateModel$ycenter)/updateModel$yscale

residual2 <- Ypred2 - Ys2
residual3 <- Ypred3 - Ys2 

mse3 <- sapply(1:nrow(Ypred2), function(i) { sum(((Ypred2[i,] - Ys2)^2) / nrow(Ypred2)) })
mse4 <- sapply(1:nrow(Ypred3), function(i) { sum(((Ypred3[i,] - Ys2)^2) / nrow(Ypred3)) })

mseData3 <- data.frame(MSE=mse3, prediction=1:nrow(Ypred2))
mseData4 <- data.frame(MSE=mse4, simulation=1:nrow(Ypred3))

minIdx3 <- which(mse3 %in% min(mse3))


YpredScaled <- Ypred2*updateModel$yscale + updateModel$ycenter
Ypred1Scaled <- (Ypred3[minIdx3,]*updateModel$yscale) + updateModel$ycenter
residualScaled <- residual3[minIdx3,]*updateModel$yscale + updateModel$ycenter
stdResidualScaled <- residual2*updateModel$yscale + updateModel$ycenter

plotData5 <- data.frame(requirementCnt=functionPoints, 
                       stdResidual=residual2[,1],
                       stdResidualScaled=stdResidualScaled[,1],
                       residuals=residual3[minIdx3,],
                       residualScaled=residualScaled,
                       predictedWaitTime=YpredScaled[,1],
                       simulatedWaitTime=Ypred1Scaled,
                       actualHours=Y1)

```


```{r, echo=FALSE}

## plot the prediction

cols <- c("#000000", "#FF0000")
labels <- c("actual", "predicted")

cols2 <- c("#000000", "#FF00FF")
labels2 <- c("actual", "simulated")


pA3 <- ggplot(plotData5, aes(x=requirementCnt)) +
  geom_point(aes(y=actualHours, col="actual")) +
  geom_point(aes(y=predictedWaitTime, col="predicted"), shape=1) +
  scale_colour_manual(values=cols, labels=labels) +
  ggtitle("Original Data predicted hours on actual hours (model2)")

pA4 <- ggplot(plotData5, aes(x=requirementCnt)) +
  geom_point(aes(y=actualHours, col="actual")) +
  geom_point(aes(y=simulatedWaitTime, col="simulated"), shape=1) +
  scale_colour_manual(values=cols2, labels=labels2) +
  ggtitle("Simulated hours on actual hours (model2)")

multiplot(pA3,pA4)

```

In this case, it appears that the updated model may not be quite as accurate as the original model, and the MSE is comparatively larger for the predicted set (mse1, and scaledmse1). However in comparison of the mse generated on the simulated data these are equivalent.



```{r,echo=FALSE}

scaleF <- function(y) {
  y*updateModel$yscale + updateModel$ycenter
}

ms1 <- sum((Ypred2 - Ys)^2) / nrow(Ypred)
scaledms1 <- sum((scaleF(t(Ypred2)) - scaleF(Ys2))^2) / nrow(Ypred2)
ms2 <- sum((Ypred3[minIdx3,] - Ys2)^2) / nrow(Ypred3)
scaledms2 <- sum((scaleF(Ypred3[minIdx3,]) - scaleF(Ys2))^2) / nrow(Ypred3)

print("MSE for prediction and simulation, both unscaled and scaled")
model2Mse <- data.frame(mse1=ms1, scaledmse1=scaledms1, simmse2=ms2, simscaledmse2=scaledms2)
models <- rbind(model1Mse, model2Mse)
rownames(models) <- c("model1", "model2")
models
```

## Model Evaluation

After having trained two models the question arises, how do we know whether Model A is better than Model B or vice versa. The Mean Square Error is a point estimate method of measuring the accuracy of the model, however additional approaches are available. One such approach is the __log pointwise predictive density__ which is a measure of the sample predictive density that is proportional to the mean square error when the model is normal with con- stant variance [3]. This measure is taken as a measure of information gain given the model.
The model with the higher lppd is taken to have better predictive generalisation.

$$
lppd = log(E[p(y_i|\theta)])
$$

where $\theta_1,...,\theta_S$ are the model parameters and the distribution $p(y|\theta)$ is the joint posterior predictive distribution.

A second property is calculated to adjust for any overfitting that may be present in the model, this is known as the __widely applicable information criterion__ (WAIC). This is calculated as
$$
WAIC = 2\sum_{i=1}^N \left(\log(E[p(y_i|\theta)]) - E[\log p(y_i|\theta)]\right)
$$

the __expected log pointwise predictive density__ is then calculated as the difference between the LPPD and WAIC.

In the example below, the data set for the original and the new data is combined, and a random sample of 50 examples is taken to calculate the ELPPD and the WAIC measures.

```{r}

X2 <- rbind(data[,1:4], newData[,1:4])
Y2 <- rbind(data.frame(y=data[,5]), data.frame(y=newData[,5]))
indexes <- sample(nrow(X2), 50)
waic <- updateModel$WAIC(test, updateModel, Y2[indexes,], X2[indexes,])

```

```{r, echo=FALSE}
compare1 <- data.frame(
  modelEppd=c(waic$modelAwaic, waic$modelBwaic),
  modelAdjEppd=c(waic$modelAadjeppd1, waic$modelBadjeppd1),
  modelWaic=c(waic$modelAwaic, waic$modelBwaic)
)
rownames(compare1) <- c("model1", "model2")
print("Comparison on original and new data")
compare1

```

Based on this assessment, the second model seems to indicate that it should be the better model for use generalisation over the data set. However this results due to the larger population of data from the second data set having being generated artificially.


## References
1. Ekrem Kocaguneli , Gregory Gay , Tim Menzies , Ye Yang , Jacky W. Keung, When to use data from other projects for effort estimation, Proceedings of the IEEE/ACM international conference on Automated software engineering, September 20-24, 2010, Antwerp, Belgium, (also at https://ts.data61.csiro.au/publications/nictaabstracts/3723.pdf).

2. Box George E P,Tiao George C, Bayesian Inference in Statistical Analysis. Wiley 1992.

3. Gelman Andrew, Carlin John B, Stern Hal S, Dunson David B, Vehtari Aki, Rubin Donald B, Bayesian Data Analysis. Chapman and Hall/CRC Press Taylor and Francis Group 3rd edition, 2013

4. Deng,Kefu (2009). _The value and validity of software effort estimation models built from a multiple organization data set_ (Masters Thesis). Retrieved from http://hdl.handle.net/10292/473 


